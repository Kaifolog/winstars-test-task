cache_folder: "E:\\.cache\\huggingface" # transformers library cache dir or null

dataset: "synthetic+facts"        # synthetic, facts, synthetic+facts
model: "microsoft/deberta-v3-base"  # google-bert/bert-base-cased, FacebookAI/roberta-base , microsoft/deberta-v3-base

train:
  learning_rate: 2e-5
  batch_size: 8
  num_epochs: 5
  weight_decay: 0.01

  save_total_limit: 3
  load_best_model_at_end: false
  metric_for_best_model: "f1"
  greater_is_better: true
